hadoop与sql的关系：
	向外扩展代替向上扩展
	键值对代替关系表
	函数式编程(MapReduce)代替声明式查询(SQL)
	hadoop最适合一次写入、多次读取的数据存储需求

wordcount的multiset不能是基于内存的，要改写成基于磁盘的，否则很容易把内存用完
数据文件要存储在多台机上，否则单台机的io跟不上，再多的机器运算也没用

分区与洗牌（partition & shuffle）:
	用wordcount举例，阶段一是map,阶段二是reduce. reduce是只有一台机在运行的。
	要想阶段二以分布式运行，则要经过分区和洗牌阶段。假设阶段二26台机
	不分区是将阶段一的所有单词放在一个wordcount(multiset)中，分区后则是不同字母开头的单词放在wordcount-a、wordcount-b ..中
	洗牌则是将阶段一所有的wordcount-a迁移到机器A进行运算，wordcount-b迁移到wordcount-b中进行运算


NameNode跟踪文件的元数据——描述系统中所包含文件以及文件如何被分割为数据块
DataNode提供数据块的备份存储，并不断向NameNode报告，以保持元数据为最新状态
Secondary NameNode也是监测HDFS集群状态，与NameNode不同的是它不接收或记录HDFS的实时变化。相反，它与NameNode通信，隔一段时间获取HDFS元数据的快照。
	NameNode在2.4.1之前是单点故障，出现故障后，可以手动启动SNN，SNN保存了一部分元数据，可减少损失
JobTracker:一旦代码提交集群，JobTracker会确定执行计划，包括据顶处理哪些文件、为不同的任务分配节点监控所有任务的运行。如果任务失败，JT会重启任务
TaskTracker：负责执行由JobTracker分配的单项任务。每个从节点只有一个TT，但每个TT可以生成多个JVM来并行处理
	DataNode和TaskTracker一定是在同一一台机的
	SNN通常也独占一台服务器，不运行DataNode和TaskTracker，集群很小的时候可以合并SNN到从节点
	NameNode一般和JobTracker一台机，大集群中可以各自占一台

技巧：建立多个配置目录conf.cluster、conf.pseudo、conf.standalone，使用ln -s conf.cluster conf来切换

web管理界面：
	NameNode： http://namenode-host:50070/ 可以看到HDFS的状态和使用情况，以及每个DataNode的情况，可以浏览文件系统
	JobTracker: http://jobtracker-host:50030/ 2.x版本的端口是8088    可以监控活跃的MapReduce作业，并访问每个map和reduce任务的日志，以及某个作业的特定参数配置

MapReduce的输入输出都是以键值对的形式，键必须实现WritableComparable，值必须实现Writable接口

Mapper和Reducer必须基础MapReduceBase
Partitioner：重定向Mapper输出，即完成partitioning和shuffing
Combiner: 本地reducer，map之后先进行本地reducing以减少数据传输


在MapReduce程序中添加自己的jar包： hadoop classpath查看所有的classpath，把所需的jar包拷贝到任意一个目录中


第一部分 环境搭建：
1、这里我们搭建一个由三台机器组成的集群：
	192.168.0.1     hduser/passwd        cloud001       nn/snn/rm        
	192.168.0.2     hduser/passwd        cloud002        dn/nm           
	192.168.0.3     hduser/passwd        cloud003        dn/nm           

1.1  上面各列分别为IP、user/passwd、hostname、在cluster中充当的角色
1.2  Hostname可以在/etc/hostname中修改（ubuntu是在这个路径下，redhat稍有不同,/etc/sysconfig/network）
1.3  这里我们为每台机器新建了一个账户hduser.
	groupadd hadoop
	useradd hduser -g hadoop -p hduser
	chown -R hduser:hadoop /usr/local/hadoop
这里需要给每个账户分配sudo的权限。（切换到root账户，修改/etc/sudoers文件，增加：hduser  ALL=(ALL) ALL 或者hduser  ALL=(ALL) NOPASSWD: ALL ）

2、修改/etc/hosts 文件，增加三台机器的ip和hostname的映射关系
    192.168.0.1     cloud001
    192.168.0.2     cloud002
    192.168.0.3     cloud003

3、打通cloud001到cloud002、cloud003的SSH无密码登陆
	1、 ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
	2、 cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
	3、 ssh localhost
	进入001的.ssh目录
	scp authorized_keys hduser@cloud002:~/.ssh/ authorized_keys_from_cloud001
	进入002的.ssh目录
		
4、 安装jdk
5、关闭防火墙
	RedHat:
	/etc/init.d/iptables stop 关闭防火墙。
	chkconfig iptables off 关闭开机启动。
	Ubuntu:
	ufw disable (重启生效)
6、修改/etc/.bashrc
	export HADOOP_HOME=/usr/local/hadoop
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
	export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop	

	export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
7、 创建数据文件夹
	mkdir -p $HADOOP_HOME/tmp


第二部分 hadoop 2.2安装：
1、解压hadoop-2.2.tar.gz到/usr/local/hadoop (注意：每台机的路径都要相同)
2、修改hadoop配置
	配置文件1：hadoop-env.sh   不同机器的jdk目录可能不同
	修改JAVA_HOME值（export JAVA_HOME=/usr/java/jdk1.7.0_40）
	配置文件2：yarn-env.sh   不同机器的jdk目录可能不同
	修改JAVA_HOME值（exportJAVA_HOME=/usr/java/jdk1.7.0_40）
	配置文件3：slaves （这个文件里面保存所有slave节点）
		cloud002
		cloud003
	配置文件4：core-site.xml
		<property>
			<name>fs.default.name</name>
			<value>hdfs://cloud001:9000</value>
		</property>
		<property>
			<name>hadoop.tmp.dir</name>
			<value>/usr/local/hadoop/data/tmp</value>
		</property>

	配置文件5：hdfs-site.xml
		<property>
			<name>dfs.replication</name>
			<value>3</value>
		</property>
		<property>
			<name>dfs.permissions</name>
			<value>false</value>
		</property>

	配置文件6：mapred-site.xml
		<property>
			<name>mapreduce.framework.name</name>
			<value>yarn</value>
		</property>

	配置文件7：yarn-site.xml
		<property>
			<name>yarn.nodemanager.aux-services</name>
			<value>mapreduce_shuffle</value>
		</property>
		<property>
			<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
			<value>org.apache.hadoop.mapred.ShuffleHandler</value>
		</property>
		<property>
			<name>yarn.resourcemanager.resource-tracker.address</name>
			<value>cloud001:8025</value>
		</property>
		<property>
			<name>yarn.resourcemanager.scheduler.address</name>
			<value>cloud001:8030</value>
		</property>
		<property>
			<name>yarn.resourcemanager.address</name>
			<value>cloud001:8040</value>
		</property>
3、复制到其他节点
这里可以写一个shell脚本进行操作（有大量节点时比较方便）
cp2slave.sh：
#!/bin/bash 
scp –r /home/hduser/hadoop hduser@cloud002:/home/hduser
scp –r /home/hduser/hadoop hduser@cloud003:/home/hduser
#拷贝之后可能需要修改JAVA_HOME配置

4、启动验证
4.1 启动hadoop
	格式化namenode：./bin/hdfs namenode –format
	启动hdfs: sbin/start-dfs.sh	
	jps查看： 001有nn, snn;  002和003有dn
	启动yarn: sbin/start-yarn.sh
	jps查看： 001有nn, snn, rm;  002和003有dn,nm

查看集群状态：./bin/hdfs dfsadmin –report
查看文件块组成：  ./bin/hdfsfsck / -files -blocks	
查看HDFS:    http://192.168.0.1:50070
查看RM:    http://192.168.0.1:8088

4.2 运行示例程序
	./bin/hdfs dfs –mkdir /input
	./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar randomwriter input
	./bin/hdfs dfs -cat input/*

PS：dataNode 无法启动是配置过程中最常见的问题，主要原因是多次format namenode 造成namenode 和datanode的clusterID不一致。建议查看datanode上面的log信息。解决办法:修改每一个datanode上面的CID(位于dfs/data/current/VERSION文件夹中)使两者一致。
还有一种解决方法: clusterID不一致，namenode的cid和datanode的cid不一致，导致的原因是对namenode进行format的之后，datanode不会进行format，所以datanode里面的cid还是和format之前namenode的cid一样，解决办法是删除datanode里面的dfs.datanode.data.dir目录和tmp目录，然后再启动start-dfs.sh

