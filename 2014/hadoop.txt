hadoop与sql的关系：
	向外扩展代替向上扩展
	键值对代替关系表
	函数式编程(MapReduce)代替声明式查询(SQL)
	hadoop最适合一次写入、多次读取的数据存储需求

wordcount的multiset不能是基于内存的，要改写成基于磁盘的，否则很容易把内存用完
数据文件要存储在多台机上，否则单台机的io跟不上，再多的机器运算也没用

分区与洗牌（partition & shuffle）:
	用wordcount举例，阶段一是map,阶段二是reduce. reduce是只有一台机在运行的。
	要想阶段二以分布式运行，则要经过分区和洗牌阶段。假设阶段二26台机
	不分区是将阶段一的所有单词放在一个wordcount(multiset)中，分区后则是不同字母开头的单词放在wordcount-a、wordcount-b ..中
	洗牌则是将阶段一所有的wordcount-a迁移到机器A进行运算，wordcount-b迁移到wordcount-b中进行运算


NameNode跟踪文件的元数据——描述系统中所包含文件以及文件如何被分割为数据块
DataNode提供数据块的备份存储，并不断向NameNode报告，以保持元数据为最新状态
Secondary NameNode也是监测HDFS集群状态，与NameNode不同的是它不接收或记录HDFS的实时变化。相反，它与NameNode通信，隔一段时间获取HDFS元数据的快照。
	NameNode在2.4.1之前是单点故障，出现故障后，可以手动启动SNN，SNN保存了一部分元数据，可减少损失
JobTracker:一旦代码提交集群，JobTracker会确定执行计划，包括据顶处理哪些文件、为不同的任务分配节点监控所有任务的运行。如果任务失败，JT会重启任务
TaskTracker：负责执行由JobTracker分配的单项任务。每个从节点只有一个TT，但每个TT可以生成多个JVM来并行处理
	DataNode和TaskTracker一定是在同一一台机的
	SNN通常也独占一台服务器，不运行DataNode和TaskTracker，集群很小的时候可以合并SNN到从节点
	NameNode一般和JobTracker一台机，大集群中可以各自占一台

技巧：建立多个配置目录conf.cluster、conf.pseudo、conf.standalone，使用ln -s conf.cluster conf来切换

web管理界面：
	NameNode： http://namenode-host:50070/ 可以看到HDFS的状态和使用情况，以及每个DataNode的情况，可以浏览文件系统
	JobTracker: http://jobtracker-host:50030/ 可以监控活跃的MapReduce作业，并访问每个map和reduce任务的日志，以及某个作业的特定参数配置

MapReduce的输入输出都是以键值对的形式，键必须实现WritableComparable，值必须实现Writable接口

Mapper和Reducer必须基础MapReduceBase
Partitioner：重定向Mapper输出，即完成partitioning和shuffing


常用InputFormat：
	1. TextInputFormat 在文本文件的每一行为一个记录。键为一行的行号，而值为一行的内容
	2. KeyValueTextInputFormat 在文本文件中的每一行均为一个记录。以key.value.separator.in.input.line为分割，默认\t
	3. SequenceFileInputFormat<K, V> 用于读取序列文件的InputFormat。键和值由用户定义。专用于一个MapReduce作业和其他MapReduce作业之间传送数据
	4. NLineInputFormat 与TextInputFormat相同，但每个分片一定有N行。N在属性mapred.line.input.format.linespermap中设定，默认1
	自定义InputFormat需要实现InputFormat接口，但通常我们并不需要关注如何把文件分块getSplits，可以继承FileInputFormat抽象类，已经实现了getSplits方法。上面的常用InputFormat都是继承FileInputFormat，只有NullInputFormat不继承
	class SelfInputFormat implements InputFormat<Text, Text> {
		public RecordReader<Text, Text> getRecordReader(InputSplit splict, JobConf conf, Reporter reporter)
				throws IOException {
			return null;
		}

		public InputSplit[] getSplits(JobConf conf, int numSplits) throws IOException {
			return null;
		}
	}

常见OutputFormat：默认TextOutputFormat
	1. TextOutputFormat<K,V> 将每个记录写成一行文本。键和值以字符串的形式写入，以mapred.textoutputforamt.seperator分隔，默认\t
	2. SequenceFileOutputFormat<K,V> 与SequenceFileInputFormat配合使用
	3. NullOutputFormat<K, V> 无输出

hadoop 0.2 API的改变：
	1. org.apache.hadoop.mapred的许多类移到org.apache.hadoop.mapreduce,类库移到org.apache.hadoop.mapreduce.lib
	2. context替换OutputReporter、Reporter对象
	3. 新的抽象类Mapper和Reducer取代了Mapper和Reducer接口，也替换了MapReducerBase类
	4. reduce方法的values类型从Iterator变成Iterable，可以使用foreach语法
	5. JobConf和JobClient被替换成Configuration(JobConf的父类)和Job(新类)
		诸如setOutputKeyClass、setInputFormat等方法从JobConf移到Job
		提交作业从JobClient.runJob(jobconf) 变成 System.exit(job.waitForCompletion(true) ? 0 : 1);
	6. KeyValueTextInputFormat不被提供，要使用TextInputFormat。 后来的版本又支持了

hadoop streaming
	使用其他语言来编写MapReduce，只要遵循UNIX的标准输入、输出就行，主要用于编写简单，短小的MapReduce程序

1. 通过unix命令使用streaming
 bin/hadoop jar contrib/streaming/hadoop-0.19.1-streaming.jar
	-input input/cite75_99.txt
	-output output
	-mapper 'cut -f 2 -d ,' #以逗号分隔，去第二列
	-reducer 'uniq'
	# -D mapred.reduce.tasks=0 #也支持GenericOptionsParser
2. 通过脚本使用streaming
	RandomSample.py  #随机取10%的行
		#!/usr/bin/env python
		import sys, random
		for line in sys.stdin:
			if (random.randint(1,100) <= int(sys.argv[1])):
				print line.strip()
	bin/hadoop jar contrib/streaming/hadoop-0.19.1-streaming.jar
		-input input/cite75_99.txt
		-output output
		-mapper 'RandomSample.py 10'
		-file RandomSample.py   #需要指定脚本文件，才能拷贝到各个节点使用
		-D mapred.reduce.tasks=1
	默认使用IdentityReducer，即直接把输入进行输出
	如果reducer的数目为0，则输出文件的数目等于mapper的数目
	如果reducer数目不为0，则输出文件的数目等于reducer的数目

	求最大值满足分配率：max(X1,X2,X3,X4) = max(max(X1,X2), max(X3,X4))
	AttributeMax.py #求某列的最大值
	#!/usr/bin/env python
	import sys
	index = int(sys.argv[1])
	max = 0
	for line in sys.stdin:
		fields = line.strip().split(",")
		if fields[index].isdigit():
			val = int(fields[index])
			if (val > max):
				max = val
	else:
		print max
	bin/hadoop jar contrib/streaming/hadoop-0.18.1-streaming.jar
		-input input/apat63_99.txt
		-output output
		-mapper 'AttributeMax.py 8' #先求出每个mapper第9列的最大值
		-reducer 'AttributeMax.py 0' #求出各个mapper中的最大值
		-file AttributeMax.py    
		-D mapred.reduce.tasks=1

聚合函数的类别：
	1. 分配性 
		最小值、总和、计算
	2. 代数型
		平均值、方差
	3. 全集型
		中值、k个最小值，k个最大值

streaming是如何完成partition和shuffing的。分区是在reducer中完成的，reducer把所有记录进行排序，reduce方法读到相同的key都连续的，从而达到<k2, list(v2)>的目的

求国家-专利声明数的平均值
	AverageByAttributeMapper.py:
	#!/usr/bin/env python
	import sys
	#一次读取一行，而不像java中分好key, value
	for line in sys.stdin:
		fields = line.split(",")
		if (fields[8] and fields[8].isdigit()):
			print fields[4][1:-1] + "\t" + fields[8]
	--------------------
	 AverageByAttributeReducer.py：
	 #!/usr/bin/env python
	import sys
	(last_key, sum, count) = (None, 0.0, 0)
	#一次读取一行，而不像java中分好key, value
	for line in sys.stdin:
		(key, val) = line.split("\t")
		# streaming中的reducer会排好序，所以可以判断key是否是上一个达到java中<k2, list<v2>的效果
		if last_key and last_key != key:   
			print last_key + "\t" + str(sum / count)
			(sum, count) = (0.0, 0)
		last_key = key
		sum += float(val)
	count += 1
	print last_key + "\t" + str(sum / count)
	------------------------------
	bin/hadoop jar contrib/streaming/hadoop-0.19.1-streaming.jar
 	  -input input/apat63_99.txt
 	  -output output
 	  -file playground/AverageByAttributeMapper.py
 	  -mapper 'AverageByAttributeMapper.py'
 	  -D mapred.reduce.tasks=0

3. 使用Aggregate使用streaming
	mapper输出的格式： function:key\tvalue
	function支持的类型：
		DoubleValueSum、LongValueMax、LongValueMin、LongValueSum、StringValueMax、StringValueMin、
		UniqValueCount、ValueHistogram（个数，最小值，中值，最大值，平均值，标准方差）
	----------------------------
	AttributeCount.py
	#!/usr/bin/env python
	import sys
	index = int(sys.argv[1])
	for line in sys.stdin:
		fields = line.split(",")
		print "LongValueSum:"" + fields[index] + "\t" + "1"
	----------------------------
	bin/hadoop jar contrib/streaming/hadoop-0.19.1-streaming.jar
	  -input input/apat63_99.txt
	  -output output
	  -file AttributeCount.py
	  -mapper 'AttributeCount.py 1' #求个各个年份的专利数
	  -reducer aggregate

	ValueHistogram的mapper格式是ValueHistogram: key_a\tvalue_a\t10， 如果没有\t10就为1

Combiner：	Combiner必须是一个java类
	combiner一般与reducer一样，使用combiner来计算平均值的问题在于在reduce不知道value值是几个值的和，不适用combiner的时候每个value就是一个值的和
	所以要在平均值使用combiner，首先要对mapper和reducer进行改造
	等效于 AverageByAttributeMapper.py的java类
	AverageByAttributeMapper.py:   #在value中加入了计数，
	public static class MapClass extends MapReduceBase implements Mapper<LongWritable, Text, Text, Text> {
		public void map(LongWritable key, Text value,
			OutputCollector<Text, Text> output, Reporter reporter) throws IOException {
			String fields[] = value.toString().split(",", -20);
			String country = fields[4];
			String numClaims = fields[8];
			if (numClaims.length() > 0 && !numClaims.startsWith("\")) {
				output.collect(new Text(country), new Text(numClaims + ",1"));    
				#也可以新建一个WritableComparable来包含numClaims和计数，但这种方式更简单
			}
		}
	}
	----------------------
	public static class Reduce extends MapReduceBase implements Reducer<Text, Text, Text, DoubleWritable> {
		public void reduce(Text key, Iterator<Text> values, OutputCollector<Text, DoubleWritable> output, Reporter reporter) throws IOException {
			double sum = 0;
			int count = 0;
			while (values.hasNext()) {
				String fields[] = values.next().toString().split(",");
				sum += Double.parseDouble(fields[0]);
				count += Integer.parseInt(fields[1]);
			}
			output.collect(key, new DoubleWritable(sum/count));
		}
	}




在MapReduce程序中添加自己的jar包： hadoop classpath查看所有的classpath，把所需的jar包拷贝到任意一个目录中


第一部分 环境搭建：
1、这里我们搭建一个由三台机器组成的集群：
	192.168.0.1     hduser/passwd        cloud001       nn/snn/rm        
	192.168.0.2     hduser/passwd        cloud002        dn/nm           
	192.168.0.3     hduser/passwd        cloud003        dn/nm           

1.1  上面各列分别为IP、user/passwd、hostname、在cluster中充当的角色
1.2  Hostname可以在/etc/hostname中修改（ubuntu是在这个路径下，redhat稍有不同,/etc/sysconfig/network）
1.3  这里我们为每台机器新建了一个账户hduser.
	groupadd hadoop
	useradd hduser -g hadoop -p hduser
	chown -R hduser:hadoop /usr/local/hadoop
这里需要给每个账户分配sudo的权限。（切换到root账户，修改/etc/sudoers文件，增加：hduser  ALL=(ALL) ALL 或者hduser  ALL=(ALL) NOPASSWD: ALL ）

2、修改/etc/hosts 文件，增加三台机器的ip和hostname的映射关系
    192.168.0.1     cloud001
    192.168.0.2     cloud002
    192.168.0.3     cloud003

3、打通cloud001到cloud002、cloud003的SSH无密码登陆
	1、 ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
	2、 cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
	3、 ssh localhost
	进入001的.ssh目录
	scp authorized_keys hduser@cloud002:~/.ssh/ authorized_keys_from_cloud001
	进入002的.ssh目录
		
4、 安装jdk
5、关闭防火墙
	RedHat:
	service iptables stop 关闭防火墙。
	chkconfig iptables off 关闭开机启动。
	Ubuntu:
	ufw disable (重启生效)
6、修改/etc/.bashrc
	export HADOOP_HOME=/usr/local/hadoop
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
	export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop	

	export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
7、 创建数据文件夹
	mkdir -p $HADOOP_HOME/tmp


第二部分 hadoop 2.2安装：
1、解压hadoop-2.2.tar.gz到/usr/local/hadoop (注意：每台机的路径都要相同)
2、修改hadoop配置
	配置文件1：hadoop-env.sh   不同机器的jdk目录可能不同
	修改JAVA_HOME值（export JAVA_HOME=/usr/java/jdk1.7.0_40）
	配置文件2：yarn-env.sh   不同机器的jdk目录可能不同
	修改JAVA_HOME值（exportJAVA_HOME=/usr/java/jdk1.7.0_40）
	配置文件3：slaves （这个文件里面保存所有slave节点）
		cloud002
		cloud003
	配置文件4：core-site.xml
		<property>
			<name>fs.default.name</name>
			<value>hdfs://cloud001:9000</value>
		</property>
		<property>
			<name>hadoop.tmp.dir</name>
			<value>/home/hduser/hadoop/tmp</value>
		</property>

	配置文件5：hdfs-site.xml
		<property>
			<name>dfs.replication</name>
			<value>3</value>
		</property>
		<property>
			<name>dfs.permissions</name>
			<value>false</value>
		</property>

	配置文件6：mapred-site.xml
		<property>
			<name>mapreduce.framework.name</name>
			<value>yarn</value>
		</property>

	配置文件7：yarn-site.xml
		<property>
			<name>yarn.nodemanager.aux-services</name>
			<value>mapreduce_shuffle</value>
		</property>
		<property>
			<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
			<value>org.apache.hadoop.mapred.ShuffleHandler</value>
		</property>
		<property>
			<name>yarn.resourcemanager.resource-tracker.address</name>
			<value>cloud001:8025</value>
		</property>
		<property>
			<name>yarn.resourcemanager.scheduler.address</name>
			<value>cloud001:8030</value>
		</property>
		<property>
			<name>yarn.resourcemanager.address</name>
			<value>cloud001:8040</value>
		</property>
3、复制到其他节点
这里可以写一个shell脚本进行操作（有大量节点时比较方便）
cp2slave.sh：
#!/bin/bash 
scp -r /home/hduser/hadoop hduser@cloud002:/home/hduser
scp -r /home/hduser/hadoop hduser@cloud003:/home/hduser
#拷贝之后可能需要修改JAVA_HOME配置

4、启动验证
4.1 启动hadoop
	格式化namenode：./bin/hdfs namenode –format
	启动hdfs: sbin/start-dfs.sh	
	jps查看： 001有nn, snn;  002和003有dn
	启动yarn: sbin/start-yarn.sh
	jps查看： 001有nn, snn, rm;  002和003有dn,nm

查看集群状态：./bin/hdfs dfsadmin –report
查看文件块组成：  ./bin/hdfsfsck / -files -blocks	
查看HDFS:    http://192.168.0.1:50070
查看RM:    http://192.168.0.1:8088

4.2 运行示例程序
	./bin/hdfs dfs –mkdir /input
	./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar randomwriter input
	./bin/hdfs dfs -cat input/*

PS：dataNode 无法启动是配置过程中最常见的问题，主要原因是多次format namenode 造成namenode 和datanode的clusterID不一致。建议查看datanode上面的log信息。解决办法:修改每一个datanode上面的CID(位于dfs/data/current/VERSION文件夹中)使两者一致。
还有一种解决方法: clusterID不一致，namenode的cid和datanode的cid不一致，导致的原因是对namenode进行format的之后，datanode不会进行format，所以datanode里面的cid还是和format之前namenode的cid一样，解决办法是删除datanode里面的dfs.datanode.data.dir目录和tmp目录，然后再启动start-dfs.sh



echo ${JAVA_HOME//\//\\\/} #将java_home的/替换成\/
# 将hadoop-env.sh文件的${JAVA_HOME}字符替换成真实的环境变量
sed -i "s/\${JAVA_HOME}/`echo ${JAVA_HOME//\//\\\/}`/g" `echo ${HADOOP_HOME}`/etc/hadoop/hadoop-env.sh
# 将27行替换成真实java_home
sed -i "27 c  export JAVA_HOME=`echo ${JAVA_HOME//\//\\\/}`" `echo ${HADOOP_HOME}`/etc/hadoop/hadoop-env.sh


org.apache.hadoop.yarn.exceptions.YarnException: Unauthorized request to start container. 
This token is expired. current time is 1413052223092 found 1413006056540  时间没同步

*/10 * * * *    ( /usr/sbin/ntpdate pool.ntp.org ) #pool.ntp.org是互联网上的时钟服务器，也可以自己通过ntpd搭建

同步完之后看看市区是否一致，是否东八区date -R，修改市区：
cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime


添加第三方jar包的集中办法， 推荐第三种
Option 1) 将jar拷贝到所有节点的$HADOOP_HOME/lib中，也可以是其他目录，通过hadoop classpath查看任意一个目录 
Option 2) 使用选项-libjars comma_seperated_jars，提交类继承Tool以支持GenericOptions
Option 3) 打jar包的时候包含一个lib目录，存放第三方jar包
Option 4) DistributedCache.addFileToClassPath(hdfsJar, conf)把jar包加入缓存
Option 5) 打一个fat jar包，即jar包里包含所有依赖，这种方式需要把hadoop本身的jar也要打进去，使jar包变得非常大


链接MapReduce作业：
1. 顺序链接：一个MapReduce作业执行完之后调用另一个作业的driver，每个作业都创建自己的JobConf，并将当前作业的输入设置为上一个作业的输出，全部执行完再删除中间产生的数据
2. 具有复杂依赖的MapReduce链接（场景：如c要a,b执行完后才执行）：通过Job和JobController来管理，对于Job对象x和y，x.addDependingJob(y)意味着y执行完才会执行x，然后通过JobController.addJob()方法添加x,y。再调用JobController.run()来提交
3. 预处理和后处理的链接：如wordcount前过滤停用词(a、the..)，或转换一个词的不同形式为相同形式（finish finished）
这些预处理可以通过一个MR作业来完成，但是这种做法会产生中间结果会占用存储和IO，十分低效，可以使用ChainMapper和ChianReducer来简化
如: map1 | map2 | reduce | map3 | map4  #map2和reduce才是mr的核心，map1是预处理，map3,map4是后处理
ChainMapper.addMapper(job, Map1.class, LongWritable.class, Text.class, Text.class, Text.class, true, map1conf);
ChainMapper.addMapper(job, Map2.class, Text.class, Text.class, Text.class, Text.class, true, map1conf);
addMapper签名的参数是(JobConf conf, Class k1, Class v1, Class k2, Class v2, boolean byValue, JobConf mapperConf);
conf和mapperConf分别是全局和局部的配置，k1,v1,k2,v2是输入输出的键值类型
byValue，默认true,意味着按值传递，即context.write(k1, v1)之后不会改变k1, v1。如果多个mapper在相同的jvm线程中执行，
传递引用可减少数据复制从而提高效率，必须确保后续的mapper不会对k, v进行修改。


数据联结：
1. Reduce联结：因为主要的操作在reduce侧完成，所以称为reduce联结，可以通过datajoin包来完成
	customers:
		customer_id, name, phone
		1,Stephanie Leung,555-555-5555
		..
	orders:
		customer_id, order_id, price, time
		1, B,88.25,20-May-2008
		..
	联结后的数据：
	1,Stephanie Leung,555-555-5555, B,88.25,20-May-2008

	// 使用DatJoin, Mapper和Reducer要继承DataJoinMapperBase，DataJoinReducerBase
	public class DataJoin extends Configured implements Tool {
	    
	    public static class MapClass extends DataJoinMapperBase {
	        // tag用于标记数据来源，使用文件名
	        protected Text generateInputTag(String inputFile) {
	            String datasource = inputFile.split("-")[0];
	            return new Text(datasource);
	        }
	        
	        // groupkey即为数据库中的joinKey，要是Text类型，作为mapper的key
	        protected Text generateGroupKey(TaggedMapOutput aRecord) {
	            String line = ((Text) aRecord.getData()).toString();
	            String[] tokens = line.split(",");
	            String groupKey = tokens[0];
	            return new Text(groupKey);
	        }
	        
	        // TaggedMapOutput包含data和tag，用于mapper的value，还要实现Writable，所以使用TaggedWritable
	        protected TaggedMapOutput generateTaggedMapOutput(Object value) {
	            TaggedWritable retv = new TaggedWritable((Text) value);
	            retv.setTag(this.inputTag);
	            return retv;
	        }
	    }
	    
	    public static class Reduce extends DataJoinReducerBase {
	        // 通过combine来过滤数据，combine来决定使用内联结，左联结还是右联结
	        // tags的元素个数与values的一致，并且一一对应
	        protected TaggedMapOutput combine(Object[] tags, Object[] values) {
	            if (tags.length < 2) return null;  //由于使用内联结，小于2的返回空
	            String joinedStr = ""; 
	            for (int i=0; i<values.length; i++) {
	                if (i > 0) joinedStr += ",";
	                TaggedWritable tw = (TaggedWritable) values[i];
	                String line = ((Text) tw.getData()).toString();
	                String[] tokens = line.split(",", 2);
	                joinedStr += tokens[1];
	            }
	            TaggedWritable retv = new TaggedWritable(new Text(joinedStr));
	            retv.setTag((Text) tags[0]); 
	            return retv;
	        }
	    }
	    
	    //TaggedMapOutput实现了getTag和setTag，并指定了抽象方法getData
	    public static class TaggedWritable extends TaggedMapOutput {
	    
	        private Writable data;
	        
	        public TaggedWritable(Writable data) {
	            this.tag = new Text("");
	            this.data = data;
	        }
	        
	        public Writable getData() {
	            return data;
	        }
	        
	        public void write(DataOutput out) throws IOException {
	            this.tag.write(out);
	            this.data.write(out);
	        }
	        
	        public void readFields(DataInput in) throws IOException {
	            this.tag.readFields(in);
	            this.data.readFields(in);
	        }
	    }

	    // main method
	    
	}
	
2. 基于DistributedCache的复制联结：（不需要reduce）
	reduce联结非常灵活，但是效率很低，map阶段的全部数据都会在网络重排，直到reduce阶段才会做联结，并且会丢弃大部分数据
	实现map阶段联结的难点在于一个mapper正在处理的记录可能与另一个不易访问的记录进行联结
	当一个数据源的与另一个数据源相差好几个数量级，小数据源可以载入全部节点内存时，就可以实现map阶段的联结
	借用数据库的术语，叫复制联结（replicated join）,即一个表被复制到集群中的所有节点上
	使用DistributedCache.addCacheFile，文件是通过HDFS作为共享数据中心分发到各节点的，且只发给任务被调度到的节点。会保存在工作目录中，MR结束后会删除
	然后再在configure阶段通过DistributedCache.getLocalCacheFiles获取缓存文件在本地hdfs的路径，然后加载到内存
	GenericOptions的-files, -archieve(解压jar, zip..), -libjars(加入classpath)就是通过DistributedCache来完成的
扩展：可能小表的大小装不下内存，或者内存装得下但是数据量太大导致复制到各个mapper的开销过大。
所以可以使用半联结(map测过滤后在reduce侧联结)。
例如广州地区客户与订单的联结，先在map过滤出广州客户，然后分到各个map节点上，加载到内存中

3. Bloom Filter
	可能使用半联结过滤出的数据依然很大，放不下内存，这时可以使用BloomFilter的数据结构。
	这是一种集合的紧凑表示法，保证了没有漏报，但是有小概率的误报。oracle 11g也用了这种技术
	BloomFilter的主要的签名如下：
	class BloomFilter<E> implements Writable {
	    
	    private BitSet bf;
	    
	    public void add(E obj) { }
	    public boolean contains(E obj) { }
	    public void union(BloomFilter<E> other) { }
	}

	使用BloomFilter：
	public class BloomFilterMR extends Configured implements Tool {
	    
	    public static class MapClass extends MapReduceBase
	        implements Mapper<Text, Text, Text, BloomFilter<String>> {
	        
	        BloomFilter<String> bf = new BloomFilter<String>();
	        OutputCollector<Text, BloomFilter<String>> oc = null;
	        
	        public void map(Text key, Text value,
	                        OutputCollector<Text, BloomFilter<String>> output,
	                        Reporter reporter) throws IOException {
	            // 自己持有一个OutputCollector的引用，因为close方法要使用。 
	            if (oc == null) oc = output; 
	            
	            // oc.collect的问题是占用空间太大，把元素加到BloomFilter，可大大节省空间
	            bf.add(key.toString());
	        }
	        
	        public void close() throws IOException {
	        	// 输出bloomfilter.    0.2x后的cleanup方法包含context参数，所以不需要自己持有oc引用
	            oc.collect(new Text("testkey"), bf);
	        }
	    }
	    
	    public static class Reduce extends MapReduceBase
	        implements Reducer<Text, BloomFilter<String>, Text, Text> {
	        
	        JobConf job = null;
	        BloomFilter<String> bf = new BloomFilter<String>();
	        
	        public void configure(JobConf job) {
	            this.job = job;
	        }
	        
	        public void reduce(Text key, Iterator<BloomFilter<String>> values,
	                           OutputCollector<Text, Text> output,
	                           Reporter reporter) throws IOException {
	            
	            while (values.hasNext()) {
	            	// 每个mapper的分片数据都加到一个br中，在reduce阶段把这些br合并
	                bf.union((BloomFilter<String>)values.next());
	            }
	        }
	        
	        public void close() throws IOException {
	        	// 获取输出路径，然后把br输出成二进制文件
	            Path file = new Path(job.get("mapred.output.dir") +
	                                 "/bloomfilter");
	            FSDataOutputStream out = file.getFileSystem(job).create(file);
	            bf.write(out);
	            out.close(); 
	        }
	    }
	    
	    public int run(String[] args) throws Exception {
	        Configuration conf = getConf(); 
	        JobConf job = new JobConf(conf, BloomFilterMR.class);
	        
	        Path in = new Path(args[0]);
	        Path out = new Path(args[1]);
	        FileInputFormat.setInputPaths(job, in);
	        FileOutputFormat.setOutputPath(job, out);

	        job.setJobName("Bloom Filter");
	        job.setMapperClass(MapClass.class);
	        job.setReducerClass(Reduce.class);
	        job.setNumReduceTasks(1);
	        
	        job.setInputFormat(KeyValueTextInputFormat.class);
	        // 自己写文件输出，不通过collector输出
	        job.setOutputFormat(NullOutputFormat.class);
	        job.setOutputKeyClass(Text.class);
	        job.setOutputValueClass(BloomFilter.class);
	        job.set("key.value.separator.in.input.line", ",");
	        
	        JobClient.runJob(job);
	        
	        return 0;
	    }
	    
	    public static void main(String[] args) throws Exception {
	        int res = ToolRunner.run(new Configuration(), new BloomFilterMR(), args);
	        System.exit(res);
	    }
	}


日志：
	hadoop_home/logs下， namenode, snn, jt, tt 每个组件都有一个以自己命名的日志文件
	hadoop_home/logs/userlogs下，通过sysout,syserr输出的信息保存在这个目录下，每次提交作业都产生新日志文件
	作为程序员应该多关注taskTracker/nodeManager的日志，代码的异常会记录在这

杀掉作业：
	hadoop job -kill job_id #job_id可以在web控制台查看

计数器：
	Reporter.incrCounter(String group, String counter, long amount)来使用计算器，譬如要跟踪坏记录的数量，计数结果会在web控制台显示

跳过坏记录：
	hadoop支持skipping模式，如果skipping模式启用，当作业失败两次后进入该模式，跳过一定大小的数据，重新运行作业，如果再失败则再跳过一定大小，再运行
	可以通过jobconf或者配置文件设置跳过数据的大小、skipping模式的尝试次数等属性

用IsolationRunner重新运行出错任务：
	作业失败，可能通过日志并不能找到原因。可以使用IsolationRunner重新运行失败的任务。
	首先要配置属性keep.failed.tasks.files为true，然后作业失败的文件就会保存在任务失效节点的
	local_dir/taskTracker/jobcache/job_id/attempt_id/word， local_dir是mapred.local.dir
	通过设置JVM属性让JVM支持远程调试
	export HADOOP_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,address=8000"
	jvm会在8000端口侦听调试器，运行代码前等待调试器接入。可以查看IDE的文档看如何接入，可以使用jdb来接入
	重新运行失败作业：（job.xml包含了IsolationRunner所需的配置信息）
	bin/hadoop org.apache.hadoop.mapred.IsolationRunner ../job.xml

性能调优：
	1. 使用combiner
	2. 减少输入数据量，如将大的数据文件拆分成几个小的数据集
	3. 使用压缩
		mapred.compress.map.output   boolean，是否压缩
		mapred.map.output.compression.codec  Class属性，使用哪种压缩
		conf.setBoolean("mapred.compress.map.output", true);
		conf.setClass("mapred.map.output.compression.codec", GzipCodec.class, CompressionCodec.class);
		也可以使用conf.setCompressMapOutput()、setMapOutputCompressorClass()来设置
		压缩器还有DefaultCodec（扩展名.deflate），GzipCodec（.gz），BZip2Codec（.bz2，特别之处是可以分割，即使不使用序列文件）
		序列文件是用于存储键/值对的可压缩的二进制文件格式，它被设计用于支持压缩并保持可分割
		一个大的压缩文件，hadoop在读取的时候要进行解压，如果当成一个文件来解压，则失去并行性，
		如果先把它拆分块，每块进行再解压，这就能提高效率。 这就是使用序列文件的好处

		conf.setOutputFormat(SequenceFileOutputFormat.class); // 使用序列文件输出
		SequenceFileOutputFormat.setOutputCompressionType(conf, CompressionType.BLOCK); // type还有Record，每条记录压缩
		FileOutputFormat.setCompressOutput(conf, true);
		FileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);
	4. 重用JVM
		默认每个mapper,reducer任务都要用一个新的jvm来运行，而启动jvm的时间非常慢
		mapred.job.reuse.jvm.num.tasks属性指定每个jvm运行的任务数(同一个作业中)， -1表示没限制
		也可通过conf.setNumTasksToExecutePerJvm来设置
	5. 根据猜测执行来运行
		某个任务可能变得很慢，hadoop会在另一个节点执行相同的任务，其中一个任务执行完就会停止另一个任务
		默认map和reduce都是启用猜测执行的，将它关闭的情况是，多个任务产生副作用，如多个任务都要创建相同的外部文件，这是就会有冲突
		通过这两个属性为false就可关闭：
		mapred.map.tasks.speculative.execution
		mapred.reduce.tasks.speculative.execution


获取任务的特定信息：
	mapred.job.id  String  作业ID
	mapred.jar  String  作业目录中jar的位置
	job.local.dir  String  作业的本地空间
	mapred.tip.id String  任务ID
	mapred.task.id String 任务重试ID
	mapred.task.is.map boolean  是否是map任务
	mapred.task.partition int  作业内部的任务ID
	map.input.file String  Mapper读取的文件路径
	map.input.start long  当前Mapper输入分片的文件偏移量
	map.input.length long  当前Mapper输入分片的字节数
	mapred.work.output.dir 任务的工作输出目录

划分多个输出文件：
	1. 按行进行划分   MultipleOutputFormat
	 	// 按专利数据的国家进行划分，输出每个国家一个目录
		public static class PartitionByCountryMTOF extends MultipleTextOutputFormat<NullWritable,Text> {
			// 每行传入key,value来确定文件名，返回文件名
		    protected String generateFileNameForKeyValue(NullWritable key, Text value, String inputfilename) {
		        String[] arr = value.toString().split(",", -1);
		        String country = arr[4].substring(1,3);
		        return country+"/"+inputfilename;
		    }
		}
	2. 按列进行划分	  MultipleOutputs
		    public static class MapClass extends MapReduceBase
		        implements Mapper<LongWritable, Text, NullWritable, Text> {
		        
		        private MultipleOutputs mos;
		        private OutputCollector<NullWritable, Text> collector;
		        
		        public void configure(JobConf conf) {
		            mos = new MultipleOutputs(conf);
		        }
		        
		        public void map(LongWritable key, Text value,
		                        OutputCollector<NullWritable, Text> output,
		                        Reporter reporter) throws IOException {
		                        
		            String[] arr = value.toString().split(",", -1);
		            String chrono = arr[0] + "," + arr[1] + "," + arr[2];
		            String geo    = arr[0] + "," + arr[4] + "," + arr[5];
		            
		            collector = mos.getCollector("chrono", reporter);
		            collector.collect(NullWritable.get(), new Text(chrono));
		            collector = mos.getCollector("geo", reporter);
		            collector.collect(NullWritable.get(), new Text(geo));
		        }
		        
		        public void close() throws IOException {
		            mos.close();
		        }
		    }
		    
		    public int run(String[] args) throws Exception {
		        // ...
		        
		        MultipleOutputs.addNamedOutput(job, "chrono", TextOutputFormat.class, NullWritable.class, Text.class); MultipleOutputs.addNamedOutput(job, "geo", TextOutputFormat.class, NullWritable.class, Text.class);
		        JobClient.runJob(job);
		        
		        return 0;
		    }
		    // 输出chrono-m-00000、chrono-m00001 ... 以及geo-m-00000 ... #m标示mapper， r标示reducer

以数据库作为输入输出：
	使用DBInputFormat，DBOutputFormat
	
保持输出顺序：hadoop不能保证reduce输出的顺序，它只能保证排好序的输入，使用TotalOrderPartitioner可排序

管理Hadoop：
生产一般需要修改的值：第三列是推荐值
	dfs.name.dir  在NameNode的本地文件系统中，存储HDFS元数据的目录  /home/hadoop/dfs/name
	dfs.data.dir  在DataNode的本地文件系统中，存储HDFS文件块的目录  /home/hadoop/dfs/data
	mapred.system.dir  在HDFS中存储共享的MapReduce系统文件的目录   /hadoop/mapred/system
	mapred.local.dir   在TaskNode的本地文件系统中，用于存储临时数据的目录
	mapred.tasktracker.{map|reduce}.tasks.maximum   在一个TaskTracker上可同时运行map和reduce任务的最大值
	hadoop.tmp.dir     Hadoop的临时目录    /home/hadoop/tmp
	dfs.datanode.du.reserved  DataNode应该具备的最小空闲空间  1073741824
	mapred.child.java.opts    每个子任务的堆栈大小  -Xmx512m
	mapred.reduce.tasks    一个作业的reduce任务个数
	#dfs.name.dir指定多个磁盘的路径可以实现备份
	#dfs.data.dir  mapred.local.dir指定多个磁盘的路径可以有效利用并行来提高效率
	#每个TT可运行的map和reduce任务数默认是2,2. 对于一台四核机器，你可以设置3,3，加上TT和DN都算一个，总共8个
	#对于双四核你也可以设置7，7  这是基于大多数任务是IO密集型的，如果是CPU密集型的任务可以调低点
	#通常建议一个作业的reduce任务数= 工作节点数 * mapred.tasktracker.reduce.tasks.maximum * （0.95或1.75）
	#0.95让所有reduce任务马上加载，1.75让一些任务马上加载，较早执行完的再执行第二轮，从而更好的负载

系统体检：
	bin/hadoop fsck / #递归检查/目录所有文件的健康状态
	# -files -blocks -locations -racks 可以加上这些选项，查看详细信息，注意有顺序，blocks需要有files, locations需要有前面两个
	bin/hadoop dfsadmin -report #获取DataNode的情况
	bin/hadoop dfsadmin -metasave filename #获取当前NameNode的活动状态，将一部分元数据保存在logs/filename中

权限设置
	bin/hadoop fs -chmod 、 -chown、 chgrp	

配额管理： 设置目录下的文件数量，目录的大小
	bin/hadoop dfsadmin -setQuota <N> dir   # N是要设置的数字， dir是目录路径，限制目录下的文件数量
	bin/hadoop dfsadmin -setSpaceQuota <N> dir   # 空间配额，限制目录的空间大小
	可以通过-clrQuota清除配额
	bin/hadoop fs -count -q directory  #查看配额

启用回收站：
	在core-site.xml配置fs.trash.interval为1440，单位分钟，即表示删除的文件会保存在用户目录的.Trash/下1440分钟

删除DataNode：
	1. 让参数dfs.hosts.exclude指向一个排除文件，启动HDFS是空的
	2. 把要删除DataNode的IP加入排除文件，一行一个
	3. 执行bin/hadoop dfsadmin -refreshnode

增加DataNode：
	1. 在主服务器conf/slaves加上新的IP
	2. DataNode上做好配置，手动启动bin/hadoop datanode
	3. bin/start-balancer.sh  可以手动停止stop-balancer.sh  默认阈值是10%,各个DataNode相差10%内就认为是均衡的

NN与SNN：
	默认NN和SNN是同一台机，对于10台或以上的集群，应该为SNN分配单独的机器	
	NN的数据是保存在FSImage和EditLog两个文件中的，FSImage是启动时的元数据快照，EditLog是后来元数据的修改记录
	在NN关闭并重启时，会合并这两个文件到FSImage，清空EditLog. 如果NN长时间不重启，Editlog会很大，下次启动会变得很慢
	SNN的意义就是可以定时合并这两个文件，不需等到下次重启才合并。
	SNN会获取NN的数据存放在另一个目录进行合并，所以SNN保留了一定时间间隔的元数据，当NN出现故障，可以使用SNN的数据，但不是最新的
	配置SNN为另一台机：
		1. conf/masters 写上SNN的IP
		2. 在SNN节点上修改hdfs-site.xml的dfs.http.address为namenode_host:50070
		   SNN是通过此地址来获取元数据的namenode_host:50070/getimage?getimage=1  或?getedit=1

恢复失效的NameNode：
	1. 当NN宕机时，修改SNN的IP为NN的IP。
	2. SNN的fs.checkpoint.dir会保存NN的快照，默认一个小时获取一次，所以要修改SNN的dfs.name.dir为fs.checkpoint.dir目录先
	3. 在SNN节点启动start-dfs.sh
	上面这种方式的元数据并不是最新的，会有一定时间的数据丢失，解决办法是，把SNN的dfs.name.dir通过nfs挂载到NN的/mnt/hadoop-dfs下，然后在NN的fs.checkpoint.dir加入/mnt/hadoop-dfs，
	这样每次写入元数据就会写入fs.checkpoint.dir指定的所有目录
	如果没有单独的SNN节点，处于安全考虑，至少应该fs.checkpoint.dir指定另一个磁盘的目录

感知网络布局和机架的设计
	假设一个hdfs的文件副本数3，上传文件的节点会放一份，第二个副本放在第一个副本不同的机架，第三个放在与第二个同机架不同节点，如果副本数大于3，后面的副本随机放。 除了块的放置，任务的放置也是机架感知的，尽量靠近数据源

	hadoop如何知道节点属于哪个机架，是通过core-site.xml的topology.script.file.name属性指定脚本，hadoop会传入一组ip
	使用空格间隔，脚本返回一组机架名称。可以通过topology.script.number.args指定传入参数个数，为了简单设为1
	#!/bin/bash
	ipaddr=$1 
	segments='echo $ipaddr | cut --delimiter=. --fields=4'
	if [ "$segments" -lt 128 ]; then
	    echo /rack-1
	else
	    echo /rack-2
	fi
	#如果没有指定脚本，则所有节点都指向/default-rack中

多用户作业调度：
	默认采用FIFO，弊端是可能大作业占用很长时间，导致应该先执行的很多小作业等待
	通过Facebook开发的公平调度器(Fair Scheduler)和雅虎开发的容量调度器(Capacity Scheduler)，可以为作业指定优先级
	在core-site.xml指定使用公平调度器：
	<!-- 指定调度器 -->
	<property>
	  <name>mapred.jobtracker.taskScheduler</name>
	  <value>org.apache.hadoop.mapred.FairScheduler</value>
	</property> 
	<!-- 公平调度器使用的池配置文件 -->
	<property>
	  <name>mapred.fairscheduler.allocation.file</name>
	  <value>HADOOP_CONF_DIR/pools.xml</value>
	</property> 
	<!-- 可提高速度 -->
	<property>
	  <name>mapred.fairscheduler.assignmultiple</name>
	  <value>true</value>
	</property>
	<!-- 指定jconf的属性 -->
	<property>
	  <name>mapred.fairscheduler.poolnameproperty</name>
	  <value>pool.name</value>
	</property> 
	<property>
	  <name>pool.name</name>
	  <value>${user.name}</value>
	</property>
	pooms.xml:
	<?xml version="1.0"?>
	<allocations>
	  <pool name="ads">
	    <minMaps>2</minMaps>
	    <minReduces>2</minReduces>
	  </pool>
	  <pool name="hive">
	    <minMaps>2</minMaps>
	    <minReduces>2</minReduces>
	    <maxRunningJobs>2</maxRunningJobs>
	  </pool>
	  <user name="chuck">
	    <maxRunningJobs>6</maxRunningJobs>
	  </user>
	  <userMaxJobsDefault>3</userMaxJobsDefault>
	</allocations>
	定义了两个特殊的池ads和hive。 hive池一次最多运行2个作业。如果要使用这些池，作业配置中pool.name要设成ads或hive
	一个用户默认最多运行3个作业，而chuck用户最多运行6个
	如果一个作业被分配到ads池中，两个作业分配到hive池，三个作业由hduser用户提交
	则一个作业至少先满足两个map,两个reduce，另外两个作业“公平分配”hive池的两个map，两个reduce，
	剩下的资源再让6个作业“公平分配”， 优先级高的获取资源多


